{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ccf877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from minio import Minio\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae116193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio client setup \n",
    "# this data from docker-compose.yml file\n",
    "client = Minio(\n",
    "    \"localhost:9000\", \n",
    "    access_key=\"admin\",\n",
    "    secret_key=\"admin123\",\n",
    "    secure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85079a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read data from minio\n",
    "def read_from_minio(bucket, filename):\n",
    "    try:\n",
    "        obj = client.get_object(bucket, filename)\n",
    "        return pd.read_csv(obj) #return data as dataframe\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        return pd.DataFrame() # return empty DataFrame on error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea98152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to upload parquet to minio\n",
    "def upload_parquet(df, bucket, filename):\n",
    "    try:\n",
    "        # convert dataframe to parquet in memory\n",
    "        parquet_buffer = io.BytesIO()\n",
    "        df.to_parquet(parquet_buffer, index=False)\n",
    "        parquet_buffer.seek(0)\n",
    "        \n",
    "        # upload to minio\n",
    "        client.put_object(\n",
    "            bucket, \n",
    "            filename,   \n",
    "            data=parquet_buffer, \n",
    "            length=parquet_buffer.getbuffer().nbytes\n",
    "        )\n",
    "        print(f\"Uploaded clean file: {filename} to bucket: {bucket}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {filename}: {e} !!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91261b",
   "metadata": {},
   "source": [
    "# Cleanning_Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451b5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse text date\n",
    "def parse_date_text(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    x = str(x).strip()\n",
    "    # convert time to standard format if needed\n",
    "    if re.search(r'\\d{1,2}[AP]M$', x):\n",
    "        x = x[:-2] + \":00 \" + x[-2:]\n",
    "    # add space before AM/PM if missing\n",
    "    x = re.sub(r'(\\d)(AM|PM)', r'\\1 \\2', x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c25cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean date column\n",
    "def clean_date_column(df, col_name='date_time', dataset_name=\"Data\"):\n",
    "    # print head of the column before cleaning\n",
    "    print(f\"\\nCleaning '{col_name}' in {dataset_name} dataset...\")\n",
    "    \n",
    "    # apply parsing function\n",
    "    df[col_name] = df[col_name].apply(parse_date_text)\n",
    "    \n",
    "    # convert to datetime\n",
    "    #utc=True to avoid warnings about timezone because i faced error in previous runs\n",
    "    df[col_name] = pd.to_datetime(df[col_name], errors='coerce', dayfirst=True,utc=True) \n",
    "    # convert timezone-aware â†’ timezone-naive (remove timezone)\n",
    "    df[col_name] = df[col_name].dt.tz_convert(None)\n",
    "    # check for invalid dates with nat and future dates\n",
    "    invalid_count = df[col_name].isna().sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f\" Found {invalid_count} invalid date entries!!! \\nDropping these rows...\")\n",
    "        df = df.dropna(subset=[col_name]) # drop rows with invalid dates\n",
    "    else:\n",
    "        print(\" All dates are valid.\")\n",
    "        \n",
    "    # check for future dates beyond 2025\n",
    "    if len(df) > 0:\n",
    "        future_dates = df[df[col_name].dt.year > 2025]\n",
    "        if len(future_dates) > 0:\n",
    "            print(f\"Found {len(future_dates)} future date entries beyond 2025!!!\")\n",
    "            print(f\"Sample: {future_dates[col_name].head(3).tolist()}\") # print sample of future dates\n",
    "            df = df[df[col_name].dt.year <= 2025] # Keep only logical years\n",
    "        else:\n",
    "            print(\" No future dates beyond 2025 found.\")\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40369a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from Bronze layer successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read raw data from Bronze layer\n",
    "try:\n",
    "    df_weather = read_from_minio(\"bronze\", \"weather_raw.csv\")\n",
    "    df_traffic = read_from_minio(\"bronze\", \"traffic_raw.csv\")\n",
    "    print(\"Data read from Bronze layer successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading data: {e} !!!!\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0732efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw counts before cleaning to use it for reporting in final\n",
    "count_w_raw = len(df_weather)\n",
    "count_t_raw = len(df_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134eddb8",
   "metadata": {},
   "source": [
    "##### Cleanning_Weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47884fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning Weather Data...\n",
      "\n",
      "Initial Weather Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   weather_id         4736 non-null   float64\n",
      " 1   date_time          4920 non-null   object \n",
      " 2   city               4756 non-null   object \n",
      " 3   season             4692 non-null   object \n",
      " 4   temperature_c      4904 non-null   float64\n",
      " 5   humidity           4912 non-null   float64\n",
      " 6   rain_mm            4881 non-null   float64\n",
      " 7   wind_speed_kmh     4867 non-null   float64\n",
      " 8   visibility_m       4920 non-null   object \n",
      " 9   weather_condition  4163 non-null   object \n",
      "dtypes: float64(5), object(5)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\" Cleaning Weather Data...\")\n",
    "df_w = df_weather.copy()\n",
    "print(\"\\nInitial Weather Data Info:\")\n",
    "df_w.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1033d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weather_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>city</th>\n",
       "      <th>season</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>humidity</th>\n",
       "      <th>rain_mm</th>\n",
       "      <th>wind_speed_kmh</th>\n",
       "      <th>visibility_m</th>\n",
       "      <th>weather_condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6092.0</td>\n",
       "      <td>08/05/2024 02AM</td>\n",
       "      <td>London</td>\n",
       "      <td>Summer</td>\n",
       "      <td>-0.422031</td>\n",
       "      <td>33.0</td>\n",
       "      <td>42.105282</td>\n",
       "      <td>54.274155</td>\n",
       "      <td>7528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8807.0</td>\n",
       "      <td>2024-07-10T00:51Z</td>\n",
       "      <td>London</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>-2.764074</td>\n",
       "      <td>150.0</td>\n",
       "      <td>49.246241</td>\n",
       "      <td>45.528274</td>\n",
       "      <td>4981</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9809.0</td>\n",
       "      <td>2024-07-18 11:30</td>\n",
       "      <td>London</td>\n",
       "      <td>Winter</td>\n",
       "      <td>-1.384667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.974057</td>\n",
       "      <td>78.584612</td>\n",
       "      <td>50000</td>\n",
       "      <td>Rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6888.0</td>\n",
       "      <td>2024-02-25T01:49Z</td>\n",
       "      <td>London</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>33.687915</td>\n",
       "      <td>68.0</td>\n",
       "      <td>31.628876</td>\n",
       "      <td>24.903601</td>\n",
       "      <td>6067</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6933.0</td>\n",
       "      <td>2024-07-31T18:37Z</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490894</td>\n",
       "      <td>68.0</td>\n",
       "      <td>36.518187</td>\n",
       "      <td>0.642094</td>\n",
       "      <td>1881</td>\n",
       "      <td>Storm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weather_id          date_time    city  season  temperature_c  humidity  \\\n",
       "0      6092.0    08/05/2024 02AM  London  Summer      -0.422031      33.0   \n",
       "1      8807.0  2024-07-10T00:51Z  London  Autumn      -2.764074     150.0   \n",
       "2      9809.0   2024-07-18 11:30  London  Winter      -1.384667      24.0   \n",
       "3      6888.0  2024-02-25T01:49Z  London  Autumn      33.687915      68.0   \n",
       "4      6933.0  2024-07-31T18:37Z  London     NaN       0.490894      68.0   \n",
       "\n",
       "     rain_mm  wind_speed_kmh visibility_m weather_condition  \n",
       "0  42.105282       54.274155         7528               NaN  \n",
       "1  49.246241       45.528274         4981              Snow  \n",
       "2  27.974057       78.584612        50000              Rain  \n",
       "3  31.628876       24.903601         6067              Snow  \n",
       "4  36.518187        0.642094         1881             Storm  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76511c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Weather IDs...\n",
      "Removed 0 fully duplicated rows.\n",
      "Cleaning Done Successfully!\n",
      "Total rows after cleaning: 5000\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate weather IDs\n",
    "print(\"Checking Weather IDs...\")\n",
    "df_w['weather_id'] = pd.to_numeric(df_w['weather_id'], errors='coerce') # unify type\n",
    "# delete fully duplicated rows first that means duplicated because of duplicate weather IDs and all other columns\n",
    "df_w_before = len(df_w)\n",
    "df_w = df_w.drop_duplicates()\n",
    "print(f\"Removed {df_w_before - len(df_w)} fully duplicated rows.\")\n",
    "\n",
    "# find max weather_id for assigning new IDs for the duplicated IDs and non_duplicated in other columns\n",
    "max_id = df_w['weather_id'].dropna().max()\n",
    "# define the problematic rows with duplicated or null weather IDs\n",
    "problem_rows = df_w[df_w['weather_id'].isna() | df_w.duplicated(subset=['weather_id'], keep=False)]\n",
    "# iterate over problematic rows and assign new unique IDs\n",
    "for idx in problem_rows.index:\n",
    "    max_id += 1\n",
    "    df_w.at[idx, 'weather_id'] = max_id # assign new unique ID\n",
    "# confirm no more duplicates\n",
    "df_w = df_w.drop_duplicates(subset=['weather_id'])\n",
    "# convert weather_id to int\n",
    "df_w['weather_id'] = df_w['weather_id'].astype(int)\n",
    "\n",
    "print(\"Cleaning Done Successfully!\")\n",
    "print(f\"Total rows after cleaning: {len(df_w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f385d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning 'date_time' in Weather dataset...\n",
      " Found 255 invalid date entries!!! \n",
      "Dropping these rows...\n",
      " No future dates beyond 2025 found.\n"
     ]
    }
   ],
   "source": [
    "# Clean date column in weather data\n",
    "df_w = clean_date_column(df_w, 'date_time', dataset_name=\"Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420f9f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Temperature...\n",
      "Found 163 outliers! (Values outside 5 to 40)\n",
      "\n",
      "Sample: [-30.0, 60.0, 60.0, 60.0, 60.0]\n"
     ]
    }
   ],
   "source": [
    "# check and clean temperature_c column from -5 to 40\n",
    "print(\"Checking Temperature...\")\n",
    "df_w['temperature_c'] = pd.to_numeric(df_w['temperature_c'], errors='coerce') # unify type\n",
    "bad_temp = df_w[(df_w['temperature_c'] < -6) | (df_w['temperature_c'] > 40)]\n",
    "\n",
    "# Detect outliers\n",
    "if len(bad_temp) > 0:\n",
    "    print(f\"Found {len(bad_temp)} outliers! (Values outside 5 to 40)\\n\")\n",
    "    print(f\"Sample: {bad_temp['temperature_c'].head().tolist()}\")# print sample of bad temps\n",
    "else:\n",
    "    print(\" Temperature data is clean.\")\n",
    "# keep only valid temperature ranges\n",
    "df_w = df_w[(df_w['temperature_c'] >= -6) & (df_w['temperature_c'] <= 40)]\n",
    "df_w['temperature_c'] = df_w['temperature_c'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761531e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Humidity...\n",
      "Found 101 outliers! (Values outside 0-100)\n",
      "\n",
      "Sample: [150.0, 150.0, 150.0, 150.0, 150.0]\n"
     ]
    }
   ],
   "source": [
    "# check and clean humidity between 0 and 100\n",
    "print(\"Checking Humidity...\")\n",
    "df_w['humidity'] =pd.to_numeric(df_w['humidity'], errors='coerce') # unify type\n",
    "df_w['humidity']= df_w['humidity'].abs()# make sure humidity is non-negative\n",
    "bad_hum = df_w[(df_w['humidity'] < 0) | (df_w['humidity'] > 100)]\n",
    "\n",
    "# Detect outliers\n",
    "if len(bad_hum) > 0:\n",
    "    print(f\"Found {len(bad_hum)} outliers! (Values outside 0-100)\\n\")\n",
    "    print(f\"Sample: {bad_hum['humidity'].head().tolist()}\")# print sample of bad humidity\n",
    "else:\n",
    "    print(\" Humidity data is clean.\")\n",
    "# keep only valid humidity ranges\n",
    "df_w = df_w[(df_w['humidity'] >= 0) & (df_w['humidity'] <= 100)]\n",
    "df_w['humidity']= df_w['humidity'].astype(int) # unify type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4f0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Wind Speed...\n",
      "Found 96 outliers! (Speed > 200 km/h)\n",
      "\n",
      "Sample: [200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "Wind Speed cleaning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# check and clean wind_speed_kmh >200\n",
    "print(\"Checking Wind Speed...\")\n",
    "# convert to numeric\n",
    "df_w['wind_speed_kmh'] = pd.to_numeric(df_w['wind_speed_kmh'], errors='coerce')\n",
    "# ensure non-negative\n",
    "df_w['wind_speed_kmh'] = df_w['wind_speed_kmh'].abs()\n",
    "\n",
    "# detect outliers (>200 km/h)\n",
    "bad_wind = df_w[df_w['wind_speed_kmh'] >= 200]\n",
    "if len(bad_wind) > 0:\n",
    "    print(f\"Found {len(bad_wind)} outliers! (Speed > 200 km/h)\\n\")\n",
    "    print(\"Sample:\", bad_wind['wind_speed_kmh'].head().tolist())\n",
    "else:\n",
    "    print(\"Wind Speed data is clean.\")\n",
    "# remove outliers and keep only valid speeds\n",
    "df_w = df_w[df_w['wind_speed_kmh'] < 200]\n",
    "\n",
    "# convert type properly\n",
    "df_w['wind_speed_kmh'] = df_w['wind_speed_kmh'].astype(float)\n",
    "print(\"Wind Speed cleaning completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b945d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Visibility...\n",
      "Found 71 outliers! (Visibility >= 40,000m)\n",
      "Sample: [50000.0, 50000.0, 50000.0, 50000.0, 50000.0]\n"
     ]
    }
   ],
   "source": [
    "# check and clean visibility_m 50 - 10000\n",
    "print(\"Checking Visibility...\")\n",
    "df_w['visibility_m'] =pd.to_numeric(df_w['visibility_m'], errors='coerce') # unify type\n",
    "df_w['visibility_m'] = df_w['visibility_m'].abs()# make sure visibility is non-negative\n",
    "df_w['visibility_m'] = pd.to_numeric(df_w['visibility_m'], errors='coerce') # unify type\n",
    "bad_vis = df_w[(df_w['visibility_m'] < 50) | (df_w['visibility_m'] > 10000)]\n",
    "\n",
    "# Detect outliers\n",
    "if len(bad_vis) > 0:\n",
    "    print(f\"Found {len(bad_vis)} outliers! (Visibility >= 40,000m)\")\n",
    "    print(f\"Sample: {bad_vis['visibility_m'].head().tolist()}\") # print sample of bad visibility\n",
    "else:\n",
    "    print(\"Visibility data is clean.\")\n",
    "# keep only valid visibility ranges\n",
    "df_w = df_w[(df_w['visibility_m'] <= 10000) | (df_w['visibility_m'] >= 50)]\n",
    "df_w['visibility_m'] =df_w['visibility_m'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a342029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardizing Weather Conditions...\n",
      "   Found 682 non-standard weather conditions\n",
      "   Non-standard values: [nan]\n",
      "   Replaced 682 non-standard values with 'unknown'\n",
      "   Weather conditions standardized to accepted values: ['unknown', 'rain', 'snow', 'storm', 'clear', 'fog']\n",
      "   Counts: {'rain': 686, 'unknown': 682, 'storm': 682, 'fog': 672, 'snow': 650, 'clear': 608}\n"
     ]
    }
   ],
   "source": [
    "# Weather Condition Imputation and Standardization\n",
    "print(\"\\nStandardizing Weather Conditions...\")\n",
    "# convert all existing values to lowercase first\n",
    "df_w['weather_condition'] = df_w['weather_condition'].str.lower()\n",
    "# define accepted weather conditions from PDF (lowercase)\n",
    "accepted_conditions = {'clear', 'rain', 'fog', 'storm', 'snow'}\n",
    "# check for non-standard values\n",
    "non_standard_mask = ~df_w['weather_condition'].isin(accepted_conditions)\n",
    "\n",
    "if non_standard_mask.any():\n",
    "    non_standard_count = non_standard_mask.sum()\n",
    "    non_standard_values = df_w[non_standard_mask]['weather_condition'].unique()\n",
    "    \n",
    "    print(f\"   Found {non_standard_count} non-standard weather conditions\")\n",
    "    print(f\"   Non-standard values: {list(non_standard_values)}\")\n",
    "    \n",
    "    # Replace non-standard values with 'unknown'\n",
    "    df_w.loc[non_standard_mask, 'weather_condition'] = 'unknown'\n",
    "    print(f\"   Replaced {non_standard_count} non-standard values with 'unknown'\")\n",
    "\n",
    "# handle missing values\n",
    "nan_count = df_w['weather_condition'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"Found {nan_count} missing weather conditions!!!! \\nfilling with 'unknown'...\")\n",
    "    df_w['weather_condition'] = df_w['weather_condition'].fillna('unknown')\n",
    "\n",
    "# final validation\n",
    "final_values = df_w['weather_condition'].unique()\n",
    "final_counts = df_w['weather_condition'].value_counts()\n",
    "\n",
    "print(f\"   Weather conditions standardized to accepted values: {list(final_values)}\")\n",
    "print(f\"   Counts: {dict(final_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c16b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing rain values...\n",
      "Filling missing rain values with median...\n"
     ]
    }
   ],
   "source": [
    "# Rainfall Imputation\n",
    "print(\"Handling missing rain values...\")\n",
    "df_w['rain_mm'] =pd.to_numeric(df_w['rain_mm'], errors='coerce') # unify type\n",
    "df_w['rain_mm'] = df_w['rain_mm'].abs()# make sure rain is non-negative\n",
    "if df_w['rain_mm'].isna().sum() > 0:\n",
    "    print(\"Filling missing rain values with median...\") # print only if there are missing values\n",
    "    rain_median = df_w['rain_mm'].median()\n",
    "    df_w['rain_mm'] = df_w['rain_mm'].fillna(rain_median) # impute missing rain with median\n",
    "else:\n",
    "    print(\"No missing rain values found.\")\n",
    "df_w['rain_mm'] = df_w['rain_mm'].astype(float) # unify type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8719b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get season from date\n",
    "def get_season(date):\n",
    "# get month from date\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"summer\"\n",
    "    else:\n",
    "        return \"autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a82207c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing season values and unifying season names...\n",
      "Found 231 non-standard season values!!!\n",
      "Sample of non-standard values: ['rainy']\n",
      "Replaced 231 non-standard values with season from date_time\n",
      "\n",
      "   Fixed 246 missing season values\n",
      "   Season values unified to lowercase: ['autumn', 'spring', 'summer', 'winter']\n",
      "   Season column now has 3980 valid entries out of 3980 total\n"
     ]
    }
   ],
   "source": [
    "# Season Imputation and Unification\n",
    "print(\"\\nHandling missing season values and unifying season names...\")\n",
    "\n",
    "# count missing before fixing\n",
    "missing_season_before = df_w['season'].isna().sum()\n",
    "\n",
    "# convert existing season values to lowercase\n",
    "if 'season' in df_w.columns:\n",
    "    df_w['season'] = df_w['season'].str.lower()\n",
    "\n",
    "# fill missing season values based on date_time\n",
    "df_w['season'] = df_w.apply(\n",
    "    lambda row: get_season(row['date_time']) if pd.isna(row['season']) else row['season'],\n",
    "    axis=1)\n",
    "\n",
    "# check for any values that don't match our standard seasons\n",
    "standard_seasons = {'winter', 'spring', 'summer', 'autumn'}\n",
    "non_standard_mask = ~df_w['season'].isin(standard_seasons)\n",
    "\n",
    "# fix non-standard values using date_time\n",
    "if non_standard_mask.any():\n",
    "    non_standard_count = non_standard_mask.sum()\n",
    "    print(f\"Found {non_standard_count} non-standard season values!!!\")\n",
    "    print(f\"Sample of non-standard values: {df_w[non_standard_mask]['season'].unique()[:5].tolist()}\")\n",
    "    \n",
    "    # Fix non-standard values using date_time\n",
    "    df_w.loc[non_standard_mask, 'season'] = df_w[non_standard_mask].apply(\n",
    "        lambda row: get_season(row['date_time']),\n",
    "        axis=1)\n",
    "    \n",
    "    print(f\"Replaced {non_standard_count} non-standard values with season from date_time\")\n",
    "\n",
    "# convert all season values to lowercase again to ensure consistency\n",
    "df_w['season'] = df_w['season'].str.lower()\n",
    "# Step 7: Count final missing values\n",
    "missing_season_after = df_w['season'].isna().sum()\n",
    "\n",
    "print(f\"\\n   Fixed {missing_season_before - missing_season_after} missing season values\")\n",
    "print(f\"   Season values unified to lowercase: {sorted(df_w['season'].unique().tolist())}\")\n",
    "print(f\"   Season column now has {len(df_w) - missing_season_after} valid entries out of {len(df_w)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08bc32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing missing cities...\n"
     ]
    }
   ],
   "source": [
    "# City Imputation\n",
    "print(\"Fixing missing cities...\")\n",
    "df_w['city'] = df_w['city'].fillna('London') # impute missing city with London because most data is from London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3380351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air Pressure Cleaning if added!!\n",
    "if 'air_pressure_hpa' in df_w.columns:\n",
    "    print(\"\\nCleaning Air Pressure Data...\")\n",
    "    df_w['air_pressure_hpa'] =pd.numeric(df_w['air_pressure_hpa'], errors='coerce') # unify type\n",
    "    df_w['air_pressure_hpa'] = df_w['air_pressure_hpa'].abs()# make sure air pressure is non-negative\n",
    "    # remove outliers outside 950-1050 hPa \n",
    "    before_count = len(df_w)\n",
    "    outlier_mask = (df_w['air_pressure_hpa'] < 950) | (df_w['air_pressure_hpa'] > 1050)\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    # handle missing values \n",
    "    missing_before = df_w['air_pressure_hpa'].isna().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"Found {missing_before} missing pressure values!!!!\")\n",
    "        \n",
    "        # fill with median (better for pressure data)\n",
    "        pressure_median = df_w['air_pressure_hpa'].median()\n",
    "        df_w['air_pressure_hpa'] = df_w['air_pressure_hpa'].fillna(pressure_median)\n",
    "        \n",
    "        print(f\"Filled {missing_before} missing values with median: {pressure_median:.1f} hPa\")\n",
    "        missing_after = df_w['air_pressure_hpa'].isna().sum()\n",
    "        print(f\"Missing values after imputation: {missing_after}\")\n",
    "        df_w['air_pressure_hpa'] = df_w['air_pressure_hpa'].astype(float, errors='coerce')# unify type\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cd8a5",
   "metadata": {},
   "source": [
    "##### Cleanning_Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c555ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning traffic Data...\n",
      "\n",
      "Initial traffic Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   traffic_id        4754 non-null   float64\n",
      " 1   date_time         4905 non-null   object \n",
      " 2   city              4747 non-null   object \n",
      " 3   area              4191 non-null   object \n",
      " 4   vehicle_count     4869 non-null   float64\n",
      " 5   avg_speed_kmh     4890 non-null   float64\n",
      " 6   accident_count    4879 non-null   float64\n",
      " 7   congestion_level  3981 non-null   object \n",
      " 8   road_condition    4031 non-null   object \n",
      " 9   visibility_m      4864 non-null   float64\n",
      "dtypes: float64(5), object(5)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\" Cleaning traffic Data...\")\n",
    "df_t = df_traffic.copy()\n",
    "print(\"\\nInitial traffic Data Info:\")\n",
    "df_t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bae98ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traffic_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>vehicle_count</th>\n",
       "      <th>avg_speed_kmh</th>\n",
       "      <th>accident_count</th>\n",
       "      <th>congestion_level</th>\n",
       "      <th>road_condition</th>\n",
       "      <th>visibility_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10938.0</td>\n",
       "      <td>2024-02-02T15:54Z</td>\n",
       "      <td>London</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>776.0</td>\n",
       "      <td>58.704429</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10283.0</td>\n",
       "      <td>03/09/2024 12PM</td>\n",
       "      <td>London</td>\n",
       "      <td>Islington</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>15.023026</td>\n",
       "      <td>8.0</td>\n",
       "      <td>High</td>\n",
       "      <td>Dry</td>\n",
       "      <td>2890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12293.0</td>\n",
       "      <td>2024-05-25T17:13Z</td>\n",
       "      <td>London</td>\n",
       "      <td>Camden</td>\n",
       "      <td>2816.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Wet</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14068.0</td>\n",
       "      <td>2024-12-09 20:54</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.0</td>\n",
       "      <td>98.951356</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Dry</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10606.0</td>\n",
       "      <td>2024-01-02T00:39Z</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3148.0</td>\n",
       "      <td>43.926189</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   traffic_id          date_time    city       area  vehicle_count  \\\n",
       "0     10938.0  2024-02-02T15:54Z  London    Chelsea          776.0   \n",
       "1     10283.0    03/09/2024 12PM  London  Islington         2021.0   \n",
       "2     12293.0  2024-05-25T17:13Z  London     Camden         2816.0   \n",
       "3     14068.0   2024-12-09 20:54  London        NaN          178.0   \n",
       "4     10606.0  2024-01-02T00:39Z  London        NaN         3148.0   \n",
       "\n",
       "   avg_speed_kmh  accident_count congestion_level road_condition  visibility_m  \n",
       "0      58.704429             6.0           Medium            NaN        8384.0  \n",
       "1      15.023026             8.0             High            Dry        2890.0  \n",
       "2            NaN             9.0           Severe            Wet        3000.0  \n",
       "3      98.951356             3.0           Medium            Dry       50000.0  \n",
       "4      43.926189             3.0           Medium            NaN        8336.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65b7a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking traffic IDs...\n",
      "Removed 0 fully duplicated rows.\n",
      "Cleaning Done Successfully!\n",
      "Total rows after cleaning: 5000\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate traffic IDs\n",
    "print(\"Checking traffic IDs...\")\n",
    "df_t['traffic_id'] = pd.to_numeric(df_t['traffic_id'], errors='coerce') # unify type\n",
    "# delete fully duplicated rows first that means duplicated because of duplicate traffic IDs and all other columns\n",
    "df_t_before = len(df_t)\n",
    "df_t = df_t.drop_duplicates()\n",
    "print(f\"Removed {df_t_before - len(df_t)} fully duplicated rows.\")\n",
    "\n",
    "# find max traffic_id for assigning new IDs for the duplicated IDs and non_duplicated in other columns\n",
    "max_id = df_t['traffic_id'].dropna().max()\n",
    "# define the problematic rows with duplicated or null traffic IDs\n",
    "problem_rows = df_t[df_t['traffic_id'].isna() | df_t.duplicated(subset=['traffic_id'], keep=False)]\n",
    "# iterate over problematic rows and assign new unique IDs\n",
    "for idx in problem_rows.index:\n",
    "    max_id += 1\n",
    "    df_t.at[idx, 'traffic_id'] = max_id # assign new unique ID\n",
    "# confirm no more duplicates\n",
    "df_t = df_t.drop_duplicates(subset=['traffic_id'])\n",
    "# convert traffic_id to int\n",
    "df_t['traffic_id'] = df_t['traffic_id'].astype(int)\n",
    "\n",
    "print(\"Cleaning Done Successfully!\")\n",
    "print(f\"Total rows after cleaning: {len(df_t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e22fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning 'date_time' in Traffic dataset...\n",
      " Found 268 invalid date entries!!! \n",
      "Dropping these rows...\n",
      " No future dates beyond 2025 found.\n"
     ]
    }
   ],
   "source": [
    "# clean traffic date column\n",
    "df_t = clean_date_column(df_t, 'date_time', dataset_name=\"Traffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87c3360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing missing cities...\n"
     ]
    }
   ],
   "source": [
    "# Fix Missing City (Standardize to London as per PDF)\n",
    "print(\"Fixing missing cities...\")\n",
    "df_t['city'] = df_t['city'].fillna('London')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab0bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing area values...\n",
      "Found 771 missing areas!!!! \n",
      "Filling with 'unknown'...\n",
      "Sample of unique areas: ['camden', 'chelsea', 'islington', 'kensington', 'southwark', 'unknown']\n",
      "Total unique area values: 6\n"
     ]
    }
   ],
   "source": [
    "# impute missing area values with 'Unknown'\n",
    "print(\"\\nHandling missing area values...\")\n",
    "# check for missing values\n",
    "missing_area = df_t['area'].isna().sum()\n",
    "if missing_area > 0:\n",
    "    print(f\"Found {missing_area} missing areas!!!! \\nFilling with 'unknown'...\")\n",
    "    \n",
    "    # fill missing values with 'unknown'\n",
    "    df_t['area'] = df_t['area'].fillna('unknown')\n",
    "\n",
    "# convert all area values to lowercase \n",
    "df_t['area'] = df_t['area'].str.lower()\n",
    "\n",
    "# show sample of unique areas (first 10)\n",
    "unique_areas = df_t['area'].unique()\n",
    "print(f\"Sample of unique areas: {sorted(unique_areas[:10]) if len(unique_areas) > 10 else sorted(unique_areas)}\")\n",
    "print(f\"Total unique area values: {len(unique_areas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc4d6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Speed...\n",
      "Speed data is clean.\n"
     ]
    }
   ],
   "source": [
    "# check and clean avg_speed_kmh\n",
    "print(\"Checking Speed...\")\n",
    "df_t['avg_speed_kmh'] = pd.to_numeric(df_t['avg_speed_kmh'], errors='coerce') # unify type\n",
    "# correct  negatives\n",
    "df_t['avg_speed_kmh'] = df_t['avg_speed_kmh'].abs() # take absolute value\n",
    "# check for unrealistic high speeds\n",
    "bad_speed = df_t[df_t['avg_speed_kmh'] > 150]\n",
    "if len(bad_speed) > 0:\n",
    "    print(f\"Found {len(bad_speed)} excessive speeds (> 150 km/h)!\") # print sample of bad speeds\n",
    "    df_t = df_t[df_t['avg_speed_kmh'] <= 150] # Keep only valid speeds\n",
    "    df_t['avg_speed_kmh'] = df_t['avg_speed_kmh'].astype(float, errors='coerce') # unify type\n",
    "else:\n",
    "    print(\"Speed data is clean.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e06f4d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Speed...\n",
      "Filled missing speeds with mean: 60.78 km/h\n"
     ]
    }
   ],
   "source": [
    "# standardize speed values\n",
    "print(\"\\nChecking Speed...\")\n",
    "df_t['avg_speed_kmh'] = pd.to_numeric(df_t['avg_speed_kmh'], errors='coerce') # unify type\n",
    "df_t['avg_speed_kmh'] = df_t['avg_speed_kmh'].abs() # Fix negatives\n",
    "df_t = df_t[df_t['avg_speed_kmh'] <= 150] # Remove outliers\n",
    "# Impute missing speed with Mean (Average)\n",
    "speed_mean = df_t['avg_speed_kmh'].mean()\n",
    "df_t['avg_speed_kmh'] = df_t['avg_speed_kmh'].fillna(speed_mean)\n",
    "print(f\"Filled missing speeds with mean: {speed_mean:.2f} km/h\")\n",
    "df_t['avg_speed_kmh'] = df_t['avg_speed_kmh'].astype(float) # unify type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c12f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Vehicle Counts...\n",
      "Vehicle Counts cleaned successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# standardize vehicle_count values\n",
    "print(\"Checking Vehicle Counts...\")\n",
    "df_t['vehicle_count'] = pd.to_numeric(df_t['vehicle_count'], errors='coerce') # unify type\n",
    "df_t['vehicle_count'] = df_t['vehicle_count'].abs() # Fix negatives\n",
    "df_t = df_t[df_t['vehicle_count'] <= 5000] # Remove outliers that are too high\n",
    "# fill missing vehicle counts with median (safer for counts)\n",
    "veh_median = df_t['vehicle_count'].median()\n",
    "df_t['vehicle_count'] = df_t['vehicle_count'].fillna(veh_median)\n",
    "df_t['vehicle_count'] = df_t['vehicle_count'].astype(int) # unify type\n",
    "print(\"Vehicle Counts cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f61bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Accidents...\n",
      " Found 108 extreme accident counts (> 10)!!!! \n",
      "Removing them...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# standardize accident_count values\n",
    "print(\"Checking Accidents...\")\n",
    "df_t['accident_count'] = pd.to_numeric(df_t['accident_count'], errors='coerce') # unify type\n",
    "df_t['accident_count'] = df_t['accident_count'].abs() # Fix negatives\n",
    "df_t['accident_count'] = df_t['accident_count'].fillna(0)# fill missing accidents with 0\n",
    "bad_acc = df_t[df_t['accident_count'] >= 11] # detect extreme accident counts\n",
    "if len(bad_acc) > 0:\n",
    "    print(f\" Found {len(bad_acc)} extreme accident counts (> 10)!!!! \\nRemoving them...\")\n",
    "    df_t = df_t[df_t['accident_count'] <= 10] # Keep only valid accident counts and delete outliers\n",
    "else:\n",
    "    print(\"Accident data is reasonable.\")\n",
    "df_t['accident_count'] = df_t['accident_count'].astype(int) # unify type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3722fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Visibility...\n",
      " Found 107 extreme visibility!!!! \n",
      "Removing them...\n",
      "Filled missing visibility with mean: 6198.06 m\n"
     ]
    }
   ],
   "source": [
    "# standardize visibility_m values\n",
    "print(\"Checking Visibility...\")\n",
    "# Ensure it's numeric first\n",
    "df_t['visibility_m'] = pd.to_numeric(df_t['visibility_m'], errors='coerce') # unify type\n",
    "df_t['visibility_m'] = df_t['visibility_m'].abs() # convert negative values to positive \n",
    "\n",
    "bad_acc = df_t[(df_t['visibility_m'] < 50) | (df_t['visibility_m'] > 10000)] # detect extreme accident counts\n",
    "if len(bad_acc) > 0:\n",
    "    print(f\" Found {len(bad_acc)} extreme visibility!!!! \\nRemoving them...\")\n",
    "    df_t = df_t[(df_t['visibility_m'] >= 50) | (df_t['visibility_m'] <= 10000)] # Keep only valid accident counts and delete outliers\n",
    "else:\n",
    "    print(\"visibility data is reasonable.\")\n",
    "# Impute missing visibility with Mean\n",
    "vis_mean = df_t['visibility_m'].mean()\n",
    "df_t['visibility_m'] = df_t['visibility_m'].fillna(vis_mean)\n",
    "print(f\"Filled missing visibility with mean: {vis_mean:.2f} m\")\n",
    "df_t['visibility_m'] = df_t['visibility_m'].astype(int) # unify type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09fe3551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardizing Congestion Levels...\n",
      "Found 1703 non-standard congestion levels!!!\n",
      "Non-standard values: [nan, 'severe']\n",
      "   Replaced 1703 non-standard values with 'unknown'\n",
      "   Congestion levels standardized to: ['medium', 'high', 'low', 'unknown']\n",
      "   Counts: {'unknown': 1703, 'low': 875, 'high': 807, 'medium': 766}\n"
     ]
    }
   ],
   "source": [
    "# standardize congestion_level values\n",
    "print(\"\\nStandardizing Congestion Levels...\")\n",
    "# convert all existing values to lowercase first\n",
    "df_t['congestion_level'] = df_t['congestion_level'].str.lower()\n",
    "# define our standard congestion levels (lowercase)\n",
    "standard_levels = {'low', 'medium', 'high'}\n",
    "#  check for non-standard values\n",
    "non_standard_mask = ~df_t['congestion_level'].isin(standard_levels)\n",
    "\n",
    "if non_standard_mask.any():\n",
    "    non_standard_count = non_standard_mask.sum()\n",
    "    non_standard_values = df_t[non_standard_mask]['congestion_level'].unique()\n",
    "    \n",
    "    print(f\"Found {non_standard_count} non-standard congestion levels!!!\")\n",
    "    print(f\"Non-standard values: {list(non_standard_values)}\")\n",
    "    \n",
    "    # replace non-standard values with 'unknown'\n",
    "    df_t.loc[non_standard_mask, 'congestion_level'] = 'unknown'\n",
    "    print(f\"   Replaced {non_standard_count} non-standard values with 'unknown'\")\n",
    "\n",
    "# fill any remaining NaN values with 'unknown'\n",
    "nan_count = df_t['congestion_level'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"   Found {nan_count} NaN values, filling with 'unknown'\")\n",
    "    df_t['congestion_level'] = df_t['congestion_level'].fillna('unknown')\n",
    "\n",
    "#  final validation\n",
    "final_values = df_t['congestion_level'].unique()\n",
    "final_counts = df_t['congestion_level'].value_counts()\n",
    "\n",
    "print(f\"   Congestion levels standardized to: {list(final_values)}\")\n",
    "print(f\"   Counts: {dict(final_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edb8b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing Road Conditions...\n",
      "Found 817 non-standard road conditions!!!\n",
      "Non-standard values: [nan]\n",
      "Replaced 817 non-standard values with 'unknown'\n",
      "   Road conditions standardized to: ['unknown', 'dry', 'wet', 'damaged', 'snowy']\n",
      "   Counts: {'snowy': 848, 'wet': 842, 'dry': 829, 'unknown': 817, 'damaged': 815}\n"
     ]
    }
   ],
   "source": [
    "# standardize road_condition values\n",
    "print(\"Standardizing Road Conditions...\")\n",
    "\n",
    "df_t['road_condition'] = df_t['road_condition'].str.lower()\n",
    "\n",
    "# define our standard road conditions (lowercase)\n",
    "standard_conditions = {'dry', 'wet', 'snowy', 'damaged'}\n",
    "\n",
    "#  check for non-standard values\n",
    "non_standard_mask = ~df_t['road_condition'].isin(standard_conditions)\n",
    "\n",
    "if non_standard_mask.any():\n",
    "    non_standard_count = non_standard_mask.sum()\n",
    "    non_standard_values = df_t[non_standard_mask]['road_condition'].unique()\n",
    "    \n",
    "    print(f\"Found {non_standard_count} non-standard road conditions!!!\")# print only if non-standard values found\n",
    "    print(f\"Non-standard values: {list(non_standard_values)}\")# print only if non-standard values found\n",
    "    \n",
    "    #  replace non-standard values with 'unknown'\n",
    "    df_t.loc[non_standard_mask, 'road_condition'] = 'unknown'\n",
    "    print(f\"Replaced {non_standard_count} non-standard values with 'unknown'\")\n",
    "\n",
    "#  fill any remaining NaN values with 'unknown'\n",
    "nan_count = df_t['road_condition'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"   Found {nan_count} NaN values, filling with 'unknown'\")\n",
    "    df_t['road_condition'] = df_t['road_condition'].fillna('unknown')\n",
    "\n",
    "#  Final validation\n",
    "final_values = df_t['road_condition'].unique()\n",
    "final_counts = df_t['road_condition'].value_counts()\n",
    "\n",
    "print(f\"   Road conditions standardized to: {list(final_values)}\")\n",
    "print(f\"   Counts: {dict(final_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "855df754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY REPORT (PHASE 2)\n",
      "\n",
      "Weather Data:\n",
      " - Rows Before: 5000\n",
      " - Rows After:  3980\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Traffic Data:\n",
      " - Rows Before: 5000\n",
      " - Rows After:  4151\n"
     ]
    }
   ],
   "source": [
    "# the report of the cleaning phase\n",
    "print(\"DATA QUALITY REPORT (PHASE 2)\\n\")\n",
    "print(f\"Weather Data:\")\n",
    "print(f\" - Rows Before: {count_w_raw}\")\n",
    "print(f\" - Rows After:  {len(df_w)}\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"Traffic Data:\")\n",
    "print(f\" - Rows Before: {count_t_raw}\")\n",
    "print(f\" - Rows After:  {len(df_t)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43aab853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4151 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   traffic_id        4151 non-null   int32         \n",
      " 1   date_time         4151 non-null   datetime64[ns]\n",
      " 2   city              4151 non-null   object        \n",
      " 3   area              4151 non-null   object        \n",
      " 4   vehicle_count     4151 non-null   int32         \n",
      " 5   avg_speed_kmh     4151 non-null   float64       \n",
      " 6   accident_count    4151 non-null   int32         \n",
      " 7   congestion_level  4151 non-null   object        \n",
      " 8   road_condition    4151 non-null   object        \n",
      " 9   visibility_m      4151 non-null   int32         \n",
      "dtypes: datetime64[ns](1), float64(1), int32(4), object(4)\n",
      "memory usage: 291.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41c04967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3980 entries, 0 to 4998\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   weather_id         3980 non-null   int32         \n",
      " 1   date_time          3980 non-null   datetime64[ns]\n",
      " 2   city               3980 non-null   object        \n",
      " 3   season             3980 non-null   object        \n",
      " 4   temperature_c      3980 non-null   float64       \n",
      " 5   humidity           3980 non-null   int32         \n",
      " 6   rain_mm            3980 non-null   float64       \n",
      " 7   wind_speed_kmh     3980 non-null   float64       \n",
      " 8   visibility_m       3980 non-null   int32         \n",
      " 9   weather_condition  3980 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(3), int32(3), object(3)\n",
      "memory usage: 295.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_w.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03a8ca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded clean file: weather_cleaned.parquet to bucket: silver\n",
      "Uploaded clean file: traffic_cleaned.parquet to bucket: silver\n",
      "Done!!  \n",
      "Check MinIO silver bucket.\n"
     ]
    }
   ],
   "source": [
    "# Upload cleaned data to Silver layer in parquet format\n",
    "upload_parquet(df_w, \"silver\", \"weather_cleaned.parquet\")\n",
    "upload_parquet(df_t, \"silver\", \"traffic_cleaned.parquet\")\n",
    "\n",
    "print(\"Done!!  \\nCheck MinIO silver bucket.\") #YES!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22fd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
